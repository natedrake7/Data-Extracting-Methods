{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from dateutil.parser import parse\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "pd.set_option(\"display.max_rows\",None)\n",
    "pd.set_option(\"display.max_columns\",None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('books_1.Best_Books_Ever.csv') #read the data\n",
    "df = pd.DataFrame(data) #create the dataframe\n",
    "\n",
    "df = df.dropna(subset=['description','language','genres','pages','publishDate','ratingsByStars'])  #drop the null values of the needed columns from the dataframe\n",
    "print(df.length())\n",
    "df['ratingsByStars'] = df['ratingsByStars'].str.strip('[]') #remvove [] brackets\n",
    "df['ratingsByStars'] = df['ratingsByStars'].str.replace(\"'\",'') #remove the ' from each word\n",
    "df[['ratingStar5','ratingStar4','ratingStar3','ratingStar2','ratingStar1']] = df['ratingsByStars'].str.split(',',expand=True) #split the string by each comma and add the new\n",
    "#string to each of the new columns created\n",
    "\n",
    "df['genres'] = df['genres'].str.strip('[]') #remove [] brackets\n",
    "df['genreSingle'] = df['genres'].str.split(',').str[0].str.strip() #extract only the first string from the stri split method\n",
    "\n",
    "df['Year'] = pd.to_datetime(df['publishDate'], errors='coerce') #conver the column to datetime object and if we encounter error set the value to null\n",
    "df.dropna(inplace=True) #2 records had invalid publish date so we remove them from the dataframe\n",
    "df['Year'] = df['Year'].dt.year #extract the year from the dates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions for extracting data (5 in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1st graph (1.1 Question)\n",
    "plt.hist(df['rating'],color='red')  #show the rating column of dataframe in a histogram\n",
    "plt.title('Histogram for Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Occurances')\n",
    "plt.show()\n",
    "\n",
    "#2nd graph (1.2 Question)\n",
    "df['pages'] = df['pages'].astype(int)   #cast to short the dataframe treating pages as integers and not strings\n",
    "dfSortedByPages = df.sort_values('pages', ascending=False,) #sort by descending order the dataframe by the pages column\n",
    "df_top10 = dfSortedByPages.head(10)   #get only the top 10 cases\n",
    "plt.barh(df_top10['title'], df_top10['pages'], color='blue') #create the bar-chart,on x-axis the number of pages and in y axis the title of the book\n",
    "\n",
    "plt.xlabel('Pages') #add labels & title\n",
    "plt.ylabel('Title')\n",
    "plt.title('Top 10 books with most pages')\n",
    "plt.show()\n",
    "\n",
    "#3rd graph (1.3 Question)\n",
    "df['ratingStar5'] = df['ratingStar5'].astype(int)   #cast to short the dataframe treating pages as integers and not strings\n",
    "dfNew = df[df['ratingStar5'] > 1000]  #get only the entries with 10.000 and above 5-star ratings first\n",
    "dfSortedByPages = dfNew.sort_values('ratingStar5', ascending=False,) #sort by descending order the dataframe by the pages column\n",
    "df_top10 = dfSortedByPages.head(10)   #get only the top 10 cases\n",
    "plt.barh(df_top10['title'], df_top10['ratingStar5'], color='grey') #create the bar-chart,on x-axis the number of 5-star ratings and in y axis the title of the book\n",
    "\n",
    "plt.xlabel('5-star ratings (in millions)') #add labels & title\n",
    "plt.ylabel('Title')\n",
    "plt.title('Top 10 books with most 5-star ratings')\n",
    "plt.show()\n",
    "\n",
    "#4rd graph (1.5 Question)\n",
    "dfNew = df['author'].value_counts()  #distinct each author of the dataframe how many books has written\n",
    "dfNew = dfNew.head(10)   #get only the 10 top frequent authors of the dataframe\n",
    "plt.barh(dfNew.index,dfNew.values, color='green')\n",
    "\n",
    "plt.xlabel('Number of books written') #add labels & title\n",
    "plt.ylabel('Author')\n",
    "plt.title('Top 10 authors with most written books')\n",
    "plt.show()\n",
    "\n",
    "#5rd graph (1.8 Question)\n",
    "dfNew = df['language'].value_counts()  #distinct each language of the dataframe how many books are written in each\n",
    "plt.barh(dfNew.index,dfNew.values, color='black')\n",
    "\n",
    "plt.xlabel('Number') #add labels & title\n",
    "plt.ylabel('Language')\n",
    "plt.title('Books found in each language')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BookId,Description,“English” as language\n",
    "df = df[df['language'] == 'English'] #omit the books that are not written in English\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english') #create a TfidfVectorizer object to extract unigrams and bigrams without stopwords of english language\n",
    "\n",
    "# fit the vectorizer to the 'description' column of the DataFrame\n",
    "tfidf = vectorizer.fit_transform(df['description']) #transform text to numbers (fitting of vectorizer)\n",
    "cos_sim = cosine_similarity(tfidf) #calculate the similarity of each row of a DataFrame and every other row\n",
    "\n",
    "similar_rows = {}   #intializing the empty directory\n",
    "for i in range(cos_sim.shape[0]):  #loop through each book and find the 100 most similar books for each\n",
    "    idx = cos_sim[i].argsort()[-102:-2] #sort the cosine similarity scores in ascending order and get last 100 values(since we need the most similar) omiting the first one that is itself\n",
    "    idx = idx[idx != i] # exclude the current row from the list of similar rows\n",
    "    similar_rows[i] = idx.tolist() # store the indices of the similar rows in a dictionary\n",
    "\n",
    "for key, value in similar_rows.items():\n",
    "    print(f\"Book {key}: {value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = Counter(df['genreSingle']) #count how many books correspond to each genre\n",
    "\n",
    "most_common = common_words.most_common(10) #keep the 10 most common\n",
    "\n",
    "most_common_genres = [] #the most common method returns a tuple containing the genre name and\n",
    "for word,count in most_common: #the number of the occurences for each genre in the dataframe\n",
    "    most_common_genres.append(word) #so we split the tuple and keep only the genre name\n",
    "\n",
    "new_df = df.copy() #create a copy of the dataframe\n",
    "new_df = new_df[new_df['genreSingle'].apply(lambda x: any(word in x for word in most_common_genres))]\n",
    "#apply a lamdba function to keep only the books that are in the 10 most common genres\n",
    "\n",
    "columns_keep = ['genreSingle','description','bookId'] #we want to keep only these 3 columns\n",
    "new_df = new_df.drop(columns=[col for col in new_df.columns if col not in columns_keep]) #delete the columns which are not in the list above\n",
    "\n",
    "new_df['description'] = new_df['description'].str.lower() #make all words lower case\n",
    "stop  = stopwords.words('english') #initialize stop words to english language\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(stop)) #regex expression\n",
    "new_df['description'] = new_df['description'].str.replace(pat,'',regex=True) #apply regex expr\n",
    "new_df['description'] = new_df['description'].str.replace(r'\\s+',' ',regex=True)\n",
    "cleaned = []\n",
    "elements = list(new_df['description']) \n",
    "for i in elements : #iterate over all elements in the description column\n",
    "    FilteredText = re.sub('https?://[A-Za-z0-9./]+','',i) #clean also the links from each element of the column\n",
    "    FilteredText = re.sub(\"[^a-zA-Z0-9]\", \" \",FilteredText) #clean also punctuation\n",
    "    cleaned.append(re.sub(r'^RT[\\s]+', '', FilteredText))\n",
    "new_df['description'] = cleaned\n",
    "new_df['description'] = new_df['description'].apply(lambda x: x.split()) #split the text in a list of lists\n",
    "model = Word2Vec(new_df['description'],vector_size=200,window=5,min_count=2,sg=1,hs=0,negative=10,workers=4,seed=34) #initialize word2vec model\n",
    "model.train(new_df['description'],total_examples=len(new_df),epochs=20) #train it\n",
    "\n",
    "with open('W2V.pkl' ,\"wb\") as file: #open it\n",
    "    pickle.dump(model,file) #save the model\n",
    "with open('df.pkl' ,\"wb\") as file: #open it\n",
    "    pickle.dump(new_df,file) #save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df.pkl',\"rb\") as file:\n",
    "    new_df = pickle.load(file) #open dataframe\n",
    "df2 = df.copy() #to be changed\n",
    "new_df = df2\n",
    "\n",
    "vectorizer = TfidfVectorizer() #use a vectorizer on the data\n",
    "X = vectorizer.fit_transform(new_df['description'])\n",
    "\n",
    "y = new_df['genreSingle'] #split data to train test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "clf = GaussianNB() #Naive Bayes method\n",
    "clf.fit(X_train.toarray(),y_train)\n",
    "y_pred = clf.predict(X_test.toarray())\n",
    "print('Accuracy of GaussianNB classifier on training set: {:.2f}'.format(clf.score(X_train.toarray(),\n",
    "y_train)))\n",
    "print('Accuracy of GaussianNB classifier on test set: {:.2f}'.format(clf.score(X_test.toarray(), y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
