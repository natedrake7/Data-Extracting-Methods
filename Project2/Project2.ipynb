{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.parser import parse\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,cross_val_predict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pd.set_option(\"display.max_rows\",None)\n",
    "pd.set_option(\"display.max_columns\",None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('books_1.Best_Books_Ever.csv') #read the data\n",
    "df = pd.DataFrame(data) #create the dataframe\n",
    "df = df.dropna(subset=['description','language','genres','pages','publishDate','ratingsByStars'])  #drop the null values of the needed columns from the dataframe\n",
    "\n",
    "df['ratingsByStars'] = df['ratingsByStars'].str.strip('[]') #remvove [] brackets\n",
    "df['ratingsByStars'] = df['ratingsByStars'].str.replace(\"'\",'') #remove the ' from each word\n",
    "df[['ratingStar5','ratingStar4','ratingStar3','ratingStar2','ratingStar1']] = df['ratingsByStars'].str.split(',',expand=True) #split the string by each comma and add the new\n",
    "#string to each of the new columns created\n",
    "\n",
    "df['genres'] = df['genres'].str.strip('[]') #remove [] brackets\n",
    "df['genreSingle'] = df['genres'].str.split(',').str[0].str.strip() #extract only the first string from the stri split method\n",
    "df['Year'] = pd.to_datetime(df['publishDate'], errors='coerce') #conver the column to datetime object and if we encounter error set the value to null\n",
    "\n",
    "df = df[df['publishDate'].str.contains(r'\\d')] #records that have non-numeric value in publish date remove them from the dataframe\n",
    "\n",
    "df['Year'] = df['Year'].dt.year #extract the year from the dates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions for extracting data (5 in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1st graph (1.1 Question)\n",
    "plt.hist(df['rating'],color='red')  #show the rating column of dataframe in a histogram\n",
    "plt.title('Histogram for Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Occurances')\n",
    "plt.show()\n",
    "\n",
    "#2nd graph (1.2 Question)\n",
    "df['pages'] = df['pages'].astype(str) #in order to clean the page column that contains strings (18 cases found) first transform the object from int32(??) to str\n",
    "df = df[df['pages'].str.isdigit()] # clean the dataframe for column page (drop non-numeric values)\n",
    "df['pages'] = df['pages'].astype(int)   #cast to sort the dataframe treating pages as integers and not strings\n",
    "dfSortedByPages = df.sort_values('pages', ascending=False,) #sort by descending order the dataframe by the pages column\n",
    "df_top10 = dfSortedByPages.head(10)   #get only the top 10 cases\n",
    "plt.barh(df_top10['title'], df_top10['pages'], color='blue') #create the bar-chart,on x-axis the number of pages and in y axis the title of the book\n",
    "\n",
    "plt.xlabel('Pages') #add labels & title\n",
    "plt.ylabel('Title')\n",
    "plt.title('Top 10 books with most pages')\n",
    "plt.show()\n",
    "\n",
    "#3rd graph (1.3 Question)\n",
    "df['ratingStar5'] = df['ratingStar5'].replace(r'^\\s*$', 0, regex=True) #fill the whitespace values of column 'ratingStar5' with a zero digit so to be able to cast to int the column\n",
    "df['ratingStar5'] = df['ratingStar5'].astype(int)   #cast to sort the dataframe treating 'ratingStar5' column as integers and not strings\n",
    "dfNew = df[df['ratingStar5'] > 1000]  #get only the entries with 10.000 and above 5-star ratings first\n",
    "dfSortedByPages = dfNew.sort_values('ratingStar5', ascending=False,) #sort by descending order the dataframe by the pages column\n",
    "df_top10 = dfSortedByPages.head(10)   #get only the top 10 cases\n",
    "plt.barh(df_top10['title'], df_top10['ratingStar5'], color='grey') #create the bar-chart,on x-axis the number of 5-star ratings and in y axis the title of the book\n",
    "\n",
    "\n",
    "plt.xlabel('5-star ratings (in millions)') #add labels & title\n",
    "plt.ylabel('Title')\n",
    "plt.title('Top 10 books with most 5-star ratings')\n",
    "plt.show()\n",
    "\n",
    "#4rd graph (1.5 Question)\n",
    "dfNew1 = df['author'].value_counts()  #distinct each author of the dataframe how many books has written\n",
    "dfNew1 = dfNew1.head(10)   #get only the 10 top frequent authors of the dataframe\n",
    "plt.barh(dfNew1.index,dfNew1.values, color='green')\n",
    "\n",
    "plt.xlabel('Number of books written') #add labels & title\n",
    "plt.ylabel('Author')\n",
    "plt.title('Top 10 authors with most written books')\n",
    "plt.show()\n",
    "\n",
    "#5rd graph (1.8 Question)\n",
    "dfNew2 = df['language'].value_counts()  #distinct each language of the dataframe how many books are written in each\n",
    "dfNew2 = dfNew2.head(10)   #get only the 10 top most common languages of the dataframe\n",
    "plt.barh(dfNew2.index,dfNew2.values, color='black')\n",
    "\n",
    "plt.xlabel('Number') #add labels & title\n",
    "plt.ylabel('Language')\n",
    "plt.title('Books found in each language')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['language'] == 'English'] #omit the books that are not written in English\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english') #create a TfidfVectorizer object to extract unigrams and bigrams without stopwords of english language\n",
    "tfidf = vectorizer.fit_transform(df['description']) #transform text to numbers (fitting of vectorizer)\n",
    "\n",
    "def recommend(item_id, num):\n",
    "      similar_rows = {}   #intializing the empty directory\n",
    "      idx = df[df['bookId'] == item_id].index[0]     #find the index of the given Book Id in the dataframe to be able to get its similar books\n",
    "      cos_sim = cosine_similarity(tfidf[idx],tfidf) #calculate the similarity of each row of a DataFrame and every other row\n",
    "      similar_indices = cos_sim.argsort()[0][-num-1:-1][::-1] #sort the cosine similarity scores in ascending order and get last n values(since we need the most similar) omiting the first one that is itself\n",
    "      print(\"Recommending \", num,\" books similar to: \", df.iloc[idx]['title'], \n",
    "            \"\\n---------------------------------------------------------\\n\")\n",
    "      for index in similar_indices:\n",
    "            similar_rows[df.iloc[index]['title']] = [df.iloc[index]['description'], cos_sim[0][index]]    #store the n-similar books in a dictionary with the needing info\n",
    "            print(\"Recommended: \", df.iloc[index]['title'],           #print it in the format that is needed the results\n",
    "                  \"\\nDescription: \", df.iloc[index]['description'],\n",
    "                  \"\\n( score:\", cos_sim[0][index], \")\\n\\n\")\n",
    "\n",
    "#for bookId in df['bookId']:\n",
    "#      recommend(bookId,100) #example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = Counter(df['genreSingle']) #count how many books correspond to each genre\n",
    "\n",
    "most_common = common_words.most_common(10) #keep the 10 most common\n",
    "\n",
    "most_common_genres = [] #the most common method returns a tuple containing the genre name and\n",
    "for word,count in most_common: #the number of the occurences for each genre in the dataframe\n",
    "    most_common_genres.append(word) #so we split the tuple and keep only the genre name\n",
    "\n",
    "new_df = df.copy() #create a copy of the dataframe\n",
    "new_df = new_df[new_df['genreSingle'].apply(lambda x: any(word in x for word in most_common_genres))]\n",
    "#apply a lamdba function to keep only the books that are in the 10 most common genres\n",
    "\n",
    "columns_keep = ['genreSingle','description','bookId'] #we want to keep only these 3 columns\n",
    "new_df = new_df.drop(columns=[col for col in new_df.columns if col not in columns_keep]) #delete the columns which are not in the list above\n",
    "\n",
    "new_df['description'] = new_df['description'].str.lower() #make all words lower case\n",
    "stop  = stopwords.words('english') #initialize stop words to english language\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(stop)) #regex expression\n",
    "new_df['description'] = new_df['description'].str.replace(pat,'',regex=True) #apply regex expr\n",
    "new_df['description'] = new_df['description'].str.replace(r'\\s+',' ',regex=True)\n",
    "cleaned = []\n",
    "elements = list(new_df['description']) \n",
    "for i in elements : #iterate over all elements in the description column\n",
    "    FilteredText = re.sub('https?://[A-Za-z0-9./]+','',i) #clean also the links from each element of the column\n",
    "    FilteredText = re.sub(\"[^a-zA-Z0-9]\", \" \",FilteredText) #clean also punctuation\n",
    "    cleaned.append(re.sub(r'^RT[\\s]+', '', FilteredText))\n",
    "new_df['description'] = cleaned\n",
    "new_df['description'] = new_df['description'].apply(lambda x: x.split()) #split the text in a list of lists\n",
    "model = Word2Vec(new_df['description'],vector_size=200,window=5,min_count=2,sg=1,hs=0,negative=10,workers=4,seed=34) #initialize word2vec model\n",
    "model.train(new_df['description'],total_examples=len(new_df),epochs=20) #train it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('W2V.pkl' ,\"wb\") as file: #open it\n",
    "    pickle.dump(model,file) #save the model\n",
    "with open('df.pkl' ,\"wb\") as file: #open it\n",
    "    pickle.dump(new_df,file) #save the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df.pkl',\"rb\") as file:\n",
    "    new_df = pickle.load(file) #open dataframe\n",
    "with open('W2V.pkl',\"rb\") as file:\n",
    "    model = pickle.load(file) #open dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for document in new_df['description']:\n",
    "    embeddings = [model.wv[word] for word in document if word in model.wv]\n",
    "    if embeddings:\n",
    "        document_embedding = np.mean(embeddings, axis=0)  # Average the word embeddings\n",
    "    else:\n",
    "        document_embedding = np.zeros(model.vector_size)  # Use zero vector for out-of-vocabulary words\n",
    "    X.append(document_embedding)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = new_df['genreSingle'] #split data to train test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.32292389629019214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natedrake7/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.12012590431124553\n",
      "Recall: 0.12243266161922085\n",
      "F-measure: 0.10848565583662971\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB() #Naive Bayes method\n",
    "clf.fit(X_train,y_train)\n",
    "#y_pred = clf.predict(X_test)\n",
    "\n",
    "# 10-fold cross-validation with metrics for GaussianNB method\n",
    "kfold = KFold(n_splits=10,shuffle=True,random_state=42)\n",
    "scores = cross_val_score(clf,X_test,y_test,cv=kfold)\n",
    "print(\"Accuracy:\", scores.mean())\n",
    "\n",
    "y_pred = cross_val_predict(clf,X_test,y_test,cv=10)\n",
    "report = classification_report(y_test,y_pred,output_dict=True,zero_division=0)\n",
    "\n",
    "precision = report['macro avg']['precision']\n",
    "recall = report['macro avg']['recall']\n",
    "f_measure = report['macro avg']['f1-score']\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-measure:\", f_measure)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natedrake7/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1]}\n",
    "\n",
    "svm = SVC()\n",
    "grid_search = GridSearchCV(svm, param_grid, scoring='accuracy',n_jobs=-1)\n",
    "grid_search.fit(X_train,y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(best_params)\n",
    "print(best_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.36762303712859956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natedrake7/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.12012590431124553\n",
      "Recall: 0.12243266161922085\n",
      "F-measure: 0.10848565583662971\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(n_jobs=-1,n_estimators=250,max_depth=256)\n",
    "#random_forest.fit(X_train,y_train)\n",
    "#y_pred = random_forest.predict(X_test)\n",
    "\n",
    "# 10-fold cross validation with metrics for Random Forest method\n",
    "kfold = KFold(n_splits=10,shuffle=True,random_state=42)\n",
    "scores = cross_val_score(random_forest,X_test,y_test,cv=kfold)\n",
    "print(\"Accuracy:\", scores.mean())\n",
    "\n",
    "y_pred = cross_val_predict(clf,X_test,y_test,cv=10)\n",
    "report = classification_report(y_test,y_pred,output_dict=True,zero_division=0)\n",
    "\n",
    "precision = report['macro avg']['precision']\n",
    "recall = report['macro avg']['recall']\n",
    "f_measure = report['macro avg']['f1-score']\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-measure:\", f_measure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
